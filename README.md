Lovely Tensors
================

<!-- WARNING: THIS FILE WAS AUTOGENERATED! DO NOT EDIT! -->

## Install

``` sh
pip install lovely-tensors
```

## How to use

![](https://github.com/xl0/lovely-tensors/raw/master/demo.gif)

How often do you find yourself debuggin a neural network? You dump a
tensor to the cell output, and see this:

``` python
numbers
```

    tensor[3, 196, 196] n=115248 x∈[-2.118, 2.640] μ=-0.388 σ=1.073

Was it really useful?

What is the shape?  
What are the statistics?  
Are any of the values `nan` or `int`?  
Is it an image of a man holding a tench?

``` python
import lovely_tensors.tensors as lt
```

``` python
lt.PRINT_OPTS.color=True
```

``` python
# A very short tensor - no min/max
print(lt.lovely(numbers.view(-1)[:2]))
# A slightly longer tensor
print(lt.lovely(numbers.view(-1)[:6].view(2,3)))
```

    tensor[2] μ=-0.345 σ=0.012 x=[-0.354, -0.337]
    tensor[2, 3] n=6 x∈[-0.440, -0.337] μ=-0.388 σ=0.038 x=[[-0.354, -0.337, -0.405], [-0.440, -0.388, -0.405]]

``` python
lt.lovely(numbers)
```

    tensor[3, 196, 196] n=115248 x∈[-2.118, 2.640] μ=-0.388 σ=1.073

``` python
lt.lovely(numbers, depth=1)
```

    tensor[3, 196, 196] n=115248 x∈[-2.118, 2.640] μ=-0.388 σ=1.073
      tensor[196, 196] n=38416 x∈[-2.118, 2.249] μ=-0.324 σ=1.036
      tensor[196, 196] n=38416 x∈[-1.966, 2.429] μ=-0.274 σ=0.973
      tensor[196, 196] n=38416 x∈[-1.804, 2.640] μ=-0.567 σ=1.178

``` python
t = numbers.view(-1)[:12].clone()

t[0] *= 10000
t[1] /= 10000
t[2] = float('inf')
t[3] = float('-inf')
t[4] = float('nan')
t = t.reshape((2,6))

print(t)
print("\n")

# A spicy tensor
print(lt.lovely(t))

# A zero tensor
print(lt.lovely(torch.zeros(10, 10)))
```

    tensor[2, 6] n=12 x∈[-3.541e+03, -3.369e-05] μ=-393.776 σ=1.180e+03 +inf! -inf! nan!


    tensor[2, 6] n=12 x∈[-3.541e+03, -3.369e-05] μ=-393.776 σ=1.180e+03 +inf! -inf! nan!
    tensor[10, 10] all_zeros

Now the important queston - is it our man?

``` python
lt.rgb(numbers)
```

![](index_files/figure-gfm/cell-9-output-1.png)

*Maaaaybe?* Looks like someone normalized him.

``` python
in_stats = { "mean": (0.485, 0.456, 0.406),
             "std": (0.229, 0.224, 0.225) }
lt.rgb(numbers, in_stats)
```

![](index_files/figure-gfm/cell-10-output-1.png)

There can be no doubt, it’s out hero the Tenchman!

One last thing - let’s monkey-patch `torch.Tensor` for convenience.

``` python
lt.monkey_patch()
t
```

    tensor[2, 6] n=12 x∈[-3.541e+03, -3.369e-05] μ=-393.776 σ=1.180e+03 +inf! -inf! nan!

``` python
t.verbose
```

    tensor[2, 6] n=12 x∈[-3.541e+03, -3.369e-05] μ=-393.776 σ=1.180e+03 +inf! -inf! nan!
    x=[[-3.5405e+03, -3.3693e-05,         inf,        -inf,         nan, -4.0543e-01],
       [-4.2255e-01, -4.9105e-01, -5.0818e-01, -5.5955e-01, -5.4243e-01, -5.0818e-01]]

``` python
t.plain
```

    [[-3.5405e+03, -3.3693e-05,         inf,        -inf,         nan, -4.0543e-01],
     [-4.2255e-01, -4.9105e-01, -5.0818e-01, -5.5955e-01, -5.4243e-01, -5.0818e-01]]

``` python
numbers.rgb
# you can also do numbers.rgb()
```

![](index_files/figure-gfm/cell-14-output-1.png)

``` python
#per-channel stats
numbers.deeper
```

    tensor[3, 196, 196] n=115248 x∈[-2.118, 2.640] μ=-0.388 σ=1.073
      tensor[196, 196] n=38416 x∈[-2.118, 2.249] μ=-0.324 σ=1.036
      tensor[196, 196] n=38416 x∈[-1.966, 2.429] μ=-0.274 σ=0.973
      tensor[196, 196] n=38416 x∈[-1.804, 2.640] μ=-0.567 σ=1.178

``` python
# You can go even deeper if you want to
dt = torch.randn(3, 3, 5)
dt.deeper(2)
```

    tensor[3, 3, 5] n=45 x∈[-2.222, 2.239] μ=-0.271 σ=1.008
      tensor[3, 5] n=15 x∈[-1.490, 2.239] μ=-0.250 σ=0.967
        tensor[5] x∈[-1.269, 0.344] μ=-0.464 σ=0.593 x=[-0.681, -0.486, -1.269, -0.226, 0.344]
        tensor[5] x∈[-1.490, 0.568] μ=-0.478 σ=0.768 x=[-0.473, 0.568, -1.490, -0.843, -0.152]
        tensor[5] x∈[-1.355, 2.239] μ=0.193 σ=1.401 x=[0.216, -1.355, -0.822, 0.685, 2.239]
      tensor[3, 5] n=15 x∈[-1.615, 1.266] μ=-0.317 σ=0.783
        tensor[5] x∈[-1.308, 0.680] μ=-0.281 σ=0.775 x=[-0.003, -0.791, 0.680, 0.015, -1.308]
        tensor[5] x∈[-1.615, 1.266] μ=-0.473 σ=1.195 x=[-1.427, -0.746, 1.266, -1.615, 0.158]
        tensor[5] x∈[-0.588, 0.099] μ=-0.197 σ=0.258 x=[-0.074, 0.099, -0.138, -0.588, -0.286]
      tensor[3, 5] n=15 x∈[-2.222, 2.081] μ=-0.247 σ=1.282
        tensor[5] x∈[-0.906, 1.225] μ=0.120 σ=0.974 x=[-0.447, -0.369, 1.225, 1.100, -0.906]
        tensor[5] x∈[-2.222, 0.952] μ=-0.860 σ=1.304 x=[0.952, -1.464, -1.582, -2.222, 0.018]
        tensor[5] x∈[-1.857, 2.081] μ=-0.002 σ=1.546 x=[-0.894, 2.081, -0.290, -1.857, 0.948]

``` python
# A quick de-norm. Don't worry, the data stays the same.
numbers.rgb(in_stats)
```

![](index_files/figure-gfm/cell-17-output-1.png)
